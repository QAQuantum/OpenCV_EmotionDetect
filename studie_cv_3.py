# -*- coding: utf-8 -*-
"""STUDIE_CV_3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18xLcdsSYtR6it3IanrzGTYOwR6SsDwR9
"""

!pip install opencv-contrib-python

!wget https://www.dropbox.com/s/nilt43hyl1dx82k/dataset.zip?dl=0

#8

# Same problem with me and then I change it and its working!

# for keras-3.0.5 & tensorflow-2.16.1 the way for importing ImageDataGenerator is

!unzip dataset.zip?dl=0



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from keras.api.preprocessing.image import load_img, img_to_array
from keras.src.layers import  Flatten, Dense
from keras.api.models import Model
from keras.src.legacy.preprocessing.image import ImageDataGenerator
from keras.api.applications.mobilenet import MobileNet, preprocess_input
from keras.api.losses import categorical_crossentropy

# from keras.layers import Flatten, Dense
# from keras.models import Model
# from keras.preprocessing.image import ImageDataGenerator , img_to_array, load_img
# from keras.applications.mobilenet import MobileNet, preprocess_input
# from keras.losses import categorical_crossentropy
# from keras.src.legacy.preprocessing.image import ImageDataGenerator

# Working with pre trained model
base_model = MobileNet( input_shape=(224,224,3), include_top= False )

for layer in base_model.layers:
  layer.trainable = False


x = Flatten()(base_model.output)
x = Dense(units=7 , activation='softmax' )(x)

# creating our model.
model = Model(base_model.input, x)

model.compile(optimizer='adam', loss= categorical_crossentropy , metrics=['accuracy']  )

train_datagen = ImageDataGenerator(
     zoom_range = 0.2,
     shear_range = 0.2,
     horizontal_flip=True,
     rescale = 1./255
)

train_data = train_datagen.flow_from_directory(directory= "/content/train",
                                               target_size=(224,224),
                                               batch_size=32,
                                  )


train_data.class_indices

train_data

val_datagen = ImageDataGenerator(rescale = 1./255 )
#/content/train
val_data = val_datagen.flow_from_directory(directory= "/content/test",
                                           target_size=(224,224),
                                           batch_size=32,
                                  )

train_data.image_data_generator

# keras.src.legacy.preprocessing.image.DirectoryIterator
# def __init__(directory, image_data_generator, target_size=(256, 256), color_mode='rgb', classes=None, class_mode='categorical', batch_size=32, shuffle=True, seed=None, data_format=None, save_to_dir=None, save_prefix='', save_format='png', follow_links=False, subset=None, interpolation='nearest', keep_aspect_ratio=False, dtype=None)
# /usr/local/lib/python3.10/dist-packages/keras/src/legacy/preprocessing/image.py
# Iterator capable of reading images from a directory on disk.

# DEPRECATED.

# # to visualize the images in the traing data denerator

# t_img , label = train_data.next()

# #-----------------------------------------------------------------------------
# # function when called will prot the images
# def plotImages(img_arr, label):
#   """
#   input  :- images array
#   output :- plots the images
#   """
#   count = 0
#   for im, l in zip(img_arr,label) :
#     plt.imshow(im)
#     plt.title(im.shape)
#     plt.axis = False
#     plt.show()

#     count += 1
#     if count == 10:
#       break

# #-----------------------------------------------------------------------------
# # function call to plot the images
# plotImages(t_img, label)

import json
model_json = model.to_json()
with open("model.json", "w") as json_file:
  json_file.write(model_json)
  model.save_weights("model.h5")
print("Saved model to disk")

## having early stopping and model check point

from keras.api.callbacks import ModelCheckpoint, EarlyStopping #import ModelCheckpoint, EarlyStopping
# early stopping
es = EarlyStopping(monitor='val_accuracy', min_delta= 0.01 , patience= 5, verbose= 1, mode='auto')

# model check point
mc = ModelCheckpoint(filepath="EVModel.keras", monitor= 'val_accuracy', verbose= 1, save_best_only= True, mode = 'auto')

# puting call back in a list
call_back = [es, mc]

hist = model.fit(train_data,
                           steps_per_epoch= 10,
                           epochs= 20,
                           validation_data= val_data,
                           validation_steps= 8,
                           callbacks=[es,mc])

import json
model_json = model.to_json()
with open("model.json", "w") as json_file:
  json_file.write(model_json)

# Loading the best fit model
from keras.api.models import load_model
model = load_model("/content/EVModel.keras")

model.metrics

h =  hist.history
h.keys()

plt.plot(h['accuracy'])
plt.plot(h['val_accuracy'] , c = "red")
plt.title("acc vs v-acc")
plt.show()

plt.plot(h['loss'])
plt.plot(h['val_loss'] , c = "red")
plt.title("loss vs v-loss")
plt.show()

# just to map o/p values
op = dict(zip( train_data.class_indices.values(), train_data.class_indices.keys()))

op

# path for the image to see if it predics correct class
path = "/content/test/fear/PrivateTest_1161501.jpg"
img = load_img(path, target_size=(224,224) )

i = img_to_array(img)/255
input_arr = np.array([i])
input_arr.shape

pred = np.argmax(model.predict(input_arr))

print(f" the image is of {op[pred]}")

# to display the image
plt.imshow(input_arr[0])
plt.title("input image")
plt.show()

"""# NOW LOAD Model for FUTURE API reference cALL"""



#!pip uninstall opencv-python
!pip install opencv-python==3.4.18.65
!pip install opencv-contrib-python==3.4.18.65

!pip freeze

from IPython.display import display, Javascript
from google.colab.output import eval_js
from base64 import b64decode

def take_photo(filename='photo.jpg', quality=0.8):
  js = Javascript('''
    async function takePhoto(quality) {
      const div = document.createElement('div');
      const capture = document.createElement('button');
      capture.textContent = 'Capture';
      div.appendChild(capture);

      const video = document.createElement('video');
      video.style.display = 'block';
      const stream = await navigator.mediaDevices.getUserMedia({video: true});

      document.body.appendChild(div);
      div.appendChild(video);
      video.srcObject = stream;
      await video.play();

      // Resize the output to fit the video element.
      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

      // Wait for Capture to be clicked.
      await new Promise((resolve) => capture.onclick = resolve);

      const canvas = document.createElement('canvas');
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      canvas.getContext('2d').drawImage(video, 0, 0);
      stream.getVideoTracks()[0].stop();
      div.remove();
      return canvas.toDataURL('image/jpeg', quality);
    }
    ''')
  display(js)
  data = eval_js('takePhoto({})'.format(quality))
  binary = b64decode(data.split(',')[1])
  with open(filename, 'wb') as f:
    f.write(binary)
  return filename

from IPython.display import Image
try:
  filename = take_photo()
  print('Saved to {}'.format(filename))

  # Show the image which was just taken.
  display(Image(filename))
except Exception as err:
  # Errors will be thrown if the user does not have a webcam or if they do not
  # grant the page permission to access it.
  print(str(err))

!pip install --upgrade opencv-python

import os
import cv2
import numpy as np
from keras.api.preprocessing import image
import warnings
warnings.filterwarnings("ignore")
from keras.api.preprocessing.image import load_img, img_to_array
from keras.api.models import load_model
from keras.api.models import model_from_json
import matplotlib.pyplot as plt
import numpy as np

import os
os.environ["OPENCV_LOG_LEVEL"] = "debug" # value must be a string
os.environ["OPENCV_VIDEOIO_DEBUG"] = "1" # value must be a string

from google.colab.patches import cv2_imshow

#face_haar_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')

face_haar_cascade = cv2.CascadeClassifier('/content/haarcascade_frontalface_default.xml')
#eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')

face_haar_cascade.getFeatureType()

# load model
#modelCV2 = load_model("EVModel.keras")

#load model
modelCV2 = model_from_json(open("/content/model.json", "r").read())
#load weights
#model.load_weights('model.h5')
modelCV2.load_weights('/content/EVModel.keras')

modelCV2.summary()

cap = cv2.VideoCapture(0,cv2.CAP_DSHOW)
cap = cv2.VideoCapture('/content/EVSales.mp4')

# Get the default frame width and height
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

# Define the codec and create VideoWriter object
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
out = cv2.VideoWriter('output.mp4', fourcc, 20.0, (frame_width, frame_height))

out

#WORKING CODE FOR  DIRECT VIDEO SAMPLE INPUT------------

# if not cap.isOpened():
#     print("Cannot open camera")
#     exit()

# while True:
#     ret, frame = cap.read()

#     if not ret or frame is None:
#         print("Failed to grab frame. Exiting ...")
#         break

#     # Make sure frame is not empty before processing
#     if frame is not None:
#         gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
#         cv2_imshow(gray)

#     if cv2.waitKey(1) == ord('q'):
#         break

# cap.release()
# cv2.destroyAllWindows()

a,b = cap.read()

a

b

ret, test_img = cap.read()  # captures frame and returns boolean value and captured image
gray_img = cv2.cvtColor(test_img,cv2.COLOR_BGR2GRAY)#COLOR_BGR2RGB

ret

test_img

gray_img

faces_detected = face_haar_cascade.detectMultiScale(gray_img)

faces_detected = face_haar_cascade.detectMultiScale(gray_img,1.1,2)

for (x,y,w,h) in faces_detected:
  cv2.rectangle(test_img,(x,y),(x+w,y+h),(0,255,0),thickness=2)
  roi_gray=gray_img[y:y+w,x:x+h]#cropping region of interest i.e. face area from  image
  roi_gray=cv2.resize(roi_gray,(224,224))
  print(roi_gray.shape)
  img_pixels = image.img_to_array(roi_gray)
  img_pixels = np.expand_dims(img_pixels, axis = 0)
  img_pixels /= 255
  print(img_pixels.shape)



  #cv2.putText(test_img, predicted_emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

resized_img = cv2.resize(test_img, (1000, 700))
from google.colab.patches import cv2_imshow
cv2_imshow(resized_img)


cap.release()
cv2.destroyAllWindows















