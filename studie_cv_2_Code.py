# -*- coding: utf-8 -*-
"""STUDIE_CV_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q7e28ujhTwFb05BKe_pZFhL6ybKlyjlM
"""

!pip install opencv-contrib-python

!wget https://www.dropbox.com/s/nilt43hyl1dx82k/dataset.zip?dl=0

#8

# Same problem with me and then I change it and its working!

# for keras-3.0.5 & tensorflow-2.16.1 the way for importing ImageDataGenerator is

!unzip dataset.zip?dl=0

# from keras.models import Sequential
# from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D
# import os

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from keras.api.preprocessing.image import load_img, img_to_array
from keras.src.layers import   Dense, Conv2D, Dropout, Flatten, MaxPooling2D
from keras.api.models import Model, Sequential, load_model
from keras.src.legacy.preprocessing.image import ImageDataGenerator
from keras.api.applications.mobilenet import MobileNet, preprocess_input
from keras.api.losses import categorical_crossentropy
from keras.api.optimizers import Adam
# from keras.layers import Flatten, Dense
# from keras.models import Model
# from keras.preprocessing.image import ImageDataGenerator , img_to_array, load_img
# from keras.applications.mobilenet import MobileNet, preprocess_input
# from keras.losses import categorical_crossentropy
# from keras.src.legacy.preprocessing.image import ImageDataGenerator

# plots accuracy and loss curves
def plot_model_history(model_history):
    """
    Plot Accuracy and Loss curves given the model_history
    """
    fig, axs = plt.subplots(1,2,figsize=(15,5))
    # summarize history for accuracy
    axs[0].plot(range(1,len(model_history.history['accuracy'])+1),model_history.history['accuracy'])
    axs[0].plot(range(1,len(model_history.history['val_accuracy'])+1),model_history.history['val_accuracy'])
    axs[0].set_title('Model Accuracy')
    axs[0].set_ylabel('Accuracy')
    axs[0].set_xlabel('Epoch')
    axs[0].set_xticks(np.arange(1,len(model_history.history['accuracy'])+1),len(model_history.history['accuracy'])/10)
    axs[0].legend(['train', 'val'], loc='best')

    # summarize history for loss
    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])
    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])
    axs[1].set_title('Model Loss')
    axs[1].set_ylabel('Loss')
    axs[1].set_xlabel('Epoch')
    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)
    axs[1].legend(['train', 'val'], loc='best')
    fig.savefig('plot.png')
    plt.show()

# train = pd.DataFrame()
# train['image'], train['label'] = createdataframe('/content/train')

# Define data generators
train_dir = '/content/train'
val_dir = '/content/test'


batch_size = 30
num_epoch = 10

train_datagen = ImageDataGenerator(rescale=1./255)
val_datagen = ImageDataGenerator(rescale=1./255)


train_datagen = ImageDataGenerator(rescale=1./255)
val_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
        train_dir,
        target_size=(48,48),
        batch_size=batch_size,
        color_mode="grayscale",
        class_mode='categorical')

validation_generator = val_datagen.flow_from_directory(
        val_dir,
        target_size=(48,48),
        batch_size=batch_size,
        color_mode="grayscale",
        class_mode='categorical')

print(train_generator)

# Create the model
model = Sequential()

model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))
model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Flatten())
model.add(Dense(1024, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(7, activation='softmax'))
model.compile(loss='categorical_crossentropy',optimizer=Adam(learning_rate=0.001),metrics=['accuracy'])
model_info = model.fit(
            train_generator,
            steps_per_epoch=20,
            epochs=num_epoch,
            validation_data=validation_generator,validation_steps=8)
#model.save_weights('StudieCV.weights.h5')
model.save('STUDIEEV.h5')

plot_model_history(model_info)

model.load_weights('/content/StudieCV.weights.h5')

# dictionary which assigns each label an emotion (alphabetical order)
emotion_dict = {0: "Angry", 1: "Disgusted", 2: "Fearful", 3: "Happy", 4: "Neutral", 5: "Sad", 6: "Surprised"}

# # serialize model structure to JSON
# model_json = model.to_json()
# with open("EVmodel.json", "w") as json_file:
#     json_file.write(model_json)

# # plot the evolution of Loss and Acuracy on the train and validation sets

# import matplotlib.pyplot as plt

# plt.figure(figsize=(20,10))
# plt.subplot(1, 2, 1)
# plt.suptitle('Optimizer : Adam', fontsize=10)
# plt.ylabel('Loss', fontsize=16)
# plt.plot(model.history['loss'], label='Training Loss')
# plt.plot(model.history['val_loss'], label='Validation Loss')
# plt.legend(loc='upper right')

# plt.subplot(1, 2, 2)
# plt.ylabel('Accuracy', fontsize=16)
# plt.plot(model.history['acc'], label='Training Accuracy')
# plt.plot(model.history['val_acc'], label='Validation Accuracy')
# plt.legend(loc='lower right')
# plt.show()



import cv2
from google.colab.patches import cv2_imshow
emotionPayload = {}
emotionList = []

# start the webcam feed
cap = cv2.VideoCapture('/content/EVSales.mp4')
while True:
    # Find haar cascade to draw bounding box around face
    ret, frame = cap.read()

    if not ret:
        break
    #facecasc = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')
    #/content/haarcascade_frontalface_default.xml
    facecasc = cv2.CascadeClassifier('/content/haarcascade_frontalface_default.xml')
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    #print(gray)
    faces = facecasc.detectMultiScale(gray,scaleFactor=1.3, minNeighbors=5)
    #emotion_dict

    for (x, y, w, h) in faces:
        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)
        roi_gray = gray[y:y + h, x:x + w]
        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray, (48, 48)), -1), 0)
        prediction = model.predict(cropped_img)
        maxindex = int(np.argmax(prediction))
        #print(maxindex)
        print(emotion_dict[maxindex])
        emotionList.append(emotion_dict[maxindex])

        cv2.putText(frame, emotion_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)

    #####################cv2.imshow('Video', cv2.resize(frame,(1600,960),interpolation = cv2.INTER_CUBIC))

    #cv2_imshow(cv2.resize(frame,(1000,700)))
    print("--------------------------------------")
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
emotionPayload['Emotion'] = emotionList

emotionPayload

import json
model_json = model.to_json()
with open("model.json", "w") as json_file:
  json_file.write(model_json)
  model.save_weights("model.h5")
print("Saved model to disk")

## having early stopping and model check point

from keras.api.callbacks import ModelCheckpoint, EarlyStopping #import ModelCheckpoint, EarlyStopping
# early stopping
es = EarlyStopping(monitor='val_accuracy', min_delta= 0.01 , patience= 5, verbose= 1, mode='auto')

# model check point
mc = ModelCheckpoint(filepath="EVModel.keras", monitor= 'val_accuracy', verbose= 1, save_best_only= True, mode = 'auto')

# puting call back in a list
call_back = [es, mc]

hist = model.fit(train_data,
                           steps_per_epoch= 10,
                           epochs= 20,
                           validation_data= val_data,
                           validation_steps= 8,
                           callbacks=[es,mc])

import json
model_json = model.to_json()
with open("model.json", "w") as json_file:
  json_file.write(model_json)

# Loading the best fit model
from keras.api.models import load_model
model = load_model("/content/EVModel.keras")

model.metrics

h =  hist.history
h.keys()

plt.plot(h['accuracy'])
plt.plot(h['val_accuracy'] , c = "red")
plt.title("acc vs v-acc")
plt.show()

plt.plot(h['loss'])
plt.plot(h['val_loss'] , c = "red")
plt.title("loss vs v-loss")
plt.show()

# just to map o/p values
op = dict(zip( train_data.class_indices.values(), train_data.class_indices.keys()))

op

# path for the image to see if it predics correct class
path = "/content/test/fear/PrivateTest_1161501.jpg"
img = load_img(path, target_size=(224,224) )

i = img_to_array(img)/255
input_arr = np.array([i])
input_arr.shape

pred = np.argmax(model.predict(input_arr))

print(f" the image is of {op[pred]}")

# to display the image
plt.imshow(input_arr[0])
plt.title("input image")
plt.show()

"""# NOW LOAD Model for FUTURE API reference cALL"""



#!pip uninstall opencv-python
!pip install opencv-python==3.4.18.65
!pip install opencv-contrib-python==3.4.18.65

!pip freeze

from IPython.display import display, Javascript
from google.colab.output import eval_js
from base64 import b64decode

def take_photo(filename='photo.jpg', quality=0.8):
  js = Javascript('''
    async function takePhoto(quality) {
      const div = document.createElement('div');
      const capture = document.createElement('button');
      capture.textContent = 'Capture';
      div.appendChild(capture);

      const video = document.createElement('video');
      video.style.display = 'block';
      const stream = await navigator.mediaDevices.getUserMedia({video: true});

      document.body.appendChild(div);
      div.appendChild(video);
      video.srcObject = stream;
      await video.play();

      // Resize the output to fit the video element.
      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

      // Wait for Capture to be clicked.
      await new Promise((resolve) => capture.onclick = resolve);

      const canvas = document.createElement('canvas');
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      canvas.getContext('2d').drawImage(video, 0, 0);
      stream.getVideoTracks()[0].stop();
      div.remove();
      return canvas.toDataURL('image/jpeg', quality);
    }
    ''')
  display(js)
  data = eval_js('takePhoto({})'.format(quality))
  binary = b64decode(data.split(',')[1])
  with open(filename, 'wb') as f:
    f.write(binary)
  return filename

from IPython.display import Image
try:
  filename = take_photo()
  print('Saved to {}'.format(filename))

  # Show the image which was just taken.
  display(Image(filename))
except Exception as err:
  # Errors will be thrown if the user does not have a webcam or if they do not
  # grant the page permission to access it.
  print(str(err))

!pip install --upgrade opencv-python

import os
import cv2
import numpy as np
from keras.api.preprocessing import image
import warnings
warnings.filterwarnings("ignore")
from keras.api.preprocessing.image import load_img, img_to_array
from keras.api.models import load_model
from keras.api.models import model_from_json
import matplotlib.pyplot as plt
import numpy as np

import os
os.environ["OPENCV_LOG_LEVEL"] = "debug" # value must be a string
os.environ["OPENCV_VIDEOIO_DEBUG"] = "1" # value must be a string

from google.colab.patches import cv2_imshow

#face_haar_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')

face_haar_cascade = cv2.CascadeClassifier('/content/haarcascade_frontalface_default.xml')
#eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')

face_haar_cascade.getFeatureType()

# load model
#modelCV2 = load_model("EVModel.keras")

#load model
modelCV2 = model_from_json(open("/content/model.json", "r").read())
#load weights
#model.load_weights('model.h5')
modelCV2.load_weights('/content/EVModel.keras')

modelCV2.summary()

cap = cv2.VideoCapture(0,cv2.CAP_DSHOW)
cap = cv2.VideoCapture('/content/EVSales.mp4')

# Get the default frame width and height
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

# Define the codec and create VideoWriter object
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
out = cv2.VideoWriter('output.mp4', fourcc, 20.0, (frame_width, frame_height))

out

#WORKING CODE FOR  DIRECT VIDEO SAMPLE INPUT------------

# if not cap.isOpened():
#     print("Cannot open camera")
#     exit()

# while True:
#     ret, frame = cap.read()

#     if not ret or frame is None:
#         print("Failed to grab frame. Exiting ...")
#         break

#     # Make sure frame is not empty before processing
#     if frame is not None:
#         gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
#         cv2_imshow(gray)

#     if cv2.waitKey(1) == ord('q'):
#         break

# cap.release()
# cv2.destroyAllWindows()

a,b = cap.read()

a

b

ret, test_img = cap.read()  # captures frame and returns boolean value and captured image
gray_img = cv2.cvtColor(test_img,cv2.COLOR_BGR2GRAY)#COLOR_BGR2RGB

ret

test_img

gray_img

faces_detected = face_haar_cascade.detectMultiScale(gray_img)

faces_detected = face_haar_cascade.detectMultiScale(gray_img,1.1,2)

for (x,y,w,h) in faces_detected:
  cv2.rectangle(test_img,(x,y),(x+w,y+h),(0,255,0),thickness=2)
  roi_gray=gray_img[y:y+w,x:x+h]#cropping region of interest i.e. face area from  image
  roi_gray=cv2.resize(roi_gray,(224,224))
  print(roi_gray.shape)
  img_pixels = image.img_to_array(roi_gray)
  img_pixels = np.expand_dims(img_pixels, axis = 0)
  img_pixels /= 255
  print(img_pixels.shape)



  #cv2.putText(test_img, predicted_emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

resized_img = cv2.resize(test_img, (1000, 700))
from google.colab.patches import cv2_imshow
cv2_imshow(resized_img)


cap.release()
cv2.destroyAllWindows















